{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "D2-9hV030WDf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cmake in c:\\users\\ban_q\\anaconda3\\lib\\site-packages (3.28.1)\n",
      "Requirement already satisfied: dlib==19.18.0 in c:\\users\\ban_q\\anaconda3\\lib\\site-packages (19.18.0)\n",
      "Requirement already satisfied: face_recognition in c:\\users\\ban_q\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ban_q\\anaconda3\\lib\\site-packages (from face_recognition) (1.24.4)\n",
      "Requirement already satisfied: Click>=6.0 in c:\\users\\ban_q\\anaconda3\\lib\\site-packages (from face_recognition) (7.1.2)\n",
      "Requirement already satisfied: face-recognition-models>=0.3.0 in c:\\users\\ban_q\\anaconda3\\lib\\site-packages (from face_recognition) (0.3.0)\n",
      "Requirement already satisfied: dlib>=19.7 in c:\\users\\ban_q\\anaconda3\\lib\\site-packages (from face_recognition) (19.18.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\ban_q\\anaconda3\\lib\\site-packages (from face_recognition) (8.0.1)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\ban_q\\anaconda3\\lib\\site-packages (4.8.1.78)\n",
      "Requirement already satisfied: numpy>=1.17.3; python_version >= \"3.8\" in c:\\users\\ban_q\\anaconda3\\lib\\site-packages (from opencv-python) (1.24.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install cmake\n",
    "!pip install dlib==19.18.0\n",
    "!pip install face_recognition\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fHlITkV70YC5"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import face_recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdFzTm-c0itv"
   },
   "source": [
    "# Facial Recognition Using DNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FS2ilKI0ndl"
   },
   "source": [
    "We integrated a 'Facial Detection Model' as well as a 'Facial Recognition Model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCecgHhK06qj"
   },
   "source": [
    "1. Download and Load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "68i9yHbS0xqk"
   },
   "outputs": [],
   "source": [
    "face_detector_model = \"res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "face_detector_config = \"deploy.prototxt\"\n",
    "face_recognition_model = \"nn4.small2.v1.t7\"\n",
    "\n",
    "face_detector_net = cv2.dnn.readNetFromCaffe(face_detector_config, face_detector_model) #From Caffe\n",
    "face_recognizer_net = cv2.dnn.readNetFromTorch(face_recognition_model) #From pyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNGBkK4v0-xM"
   },
   "source": [
    "2. Functions to be used and store the known faces with their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6bqOzpjK05xg"
   },
   "outputs": [],
   "source": [
    "def load_known_faces(known_face_paths):\n",
    "    known_faces = []\n",
    "    for path, name in known_face_paths:\n",
    "        image = cv2.imread(path, 1)\n",
    "        image = cv2.resize(image, (96, 96))\n",
    "        face_blob = cv2.dnn.blobFromImage(image, 1.0 / 255, (96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
    "        face_recognizer_net.setInput(face_blob)\n",
    "        vec = face_recognizer_net.forward()\n",
    "        known_faces.append((vec, name))\n",
    "    return known_faces\n",
    "\n",
    "def find_closest_match(face_vec, known_faces):\n",
    "    min_dist = float('inf')\n",
    "    name = \"Unknown\"\n",
    "    for vec, known_name in known_faces:\n",
    "        dist = np.linalg.norm(face_vec - vec)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            name = known_name\n",
    "    return name\n",
    "\n",
    "known_face_paths = [\n",
    "    (\"alex.jpeg\", \"Alex\"),\n",
    "    (\"ban.jpeg\", \"Ban\"),\n",
    "    (\"hiba.jpeg\", \"Hiba\")\n",
    "]\n",
    "known_faces = load_known_faces(known_face_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QF8BrJtl1J6p"
   },
   "source": [
    "3. Start the webcam and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "847sqUR-1JBb"
   },
   "outputs": [],
   "source": [
    "# Start the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "#Webcam resolution is decreased for more optimized processing (faster)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,640) #\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "# Run face recognition every 5 frames for efficiency (since large model)\n",
    "frame_count = 0\n",
    "face_recognition_interval = 5\n",
    "last_recognized_faces = {}\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    (h, w) = frame.shape[:2]\n",
    "    if frame_count % face_recognition_interval == 0:\n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "        face_detector_net.setInput(blob)\n",
    "        detections = face_detector_net.forward()\n",
    "\n",
    "        for i in range(0, detections.shape[2]):\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "            if confidence > 0.5:\n",
    "                box = detections[0, 0, i, 3:]*np.array([w,h,w,h])\n",
    "                (startX,startY,endX,endY)= box.astype(\"int\")\n",
    "            # Extract the face ROI\n",
    "        face = frame[startY:endY, startX:endX]\n",
    "        face_blob = cv2.dnn.blobFromImage(face, 1.0 / 255, (96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
    "        face_recognizer_net.setInput(face_blob)\n",
    "        face_vec = face_recognizer_net.forward()\n",
    "\n",
    "            # Find the closest match from the known faces\n",
    "            # Caching results to avoid recalculating for each frame\n",
    "        name = last_recognized_faces.get((startX, startY, endX, endY))\n",
    "        if name is None:\n",
    "            name = find_closest_match(face_vec, known_faces)\n",
    "            last_recognized_faces[(startX, startY, endX, endY)] = name\n",
    "\n",
    "        cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 0, 255), 2)\n",
    "        cv2.putText(frame, name, (startX, startY - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "\n",
    "# Display the output\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "frame_count += 1\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vn0ME1cS2BQT"
   },
   "source": [
    "# Facial Recognition Using Python Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwC45AD82MI3"
   },
   "source": [
    "1. Load the images (dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RsjSNjnN2Kfg"
   },
   "outputs": [],
   "source": [
    "#Load the images that the model will use for recognition:\n",
    "image_person1 = face_recognition.load_image_file(\"alex.jpeg\")\n",
    "image_person2 = face_recognition.load_image_file(\"hiba.jpeg\")\n",
    "image_person3 = face_recognition.load_image_file(\"ban.jpeg\")\n",
    "image_person4 = face_recognition.load_image_file(\"ahmad.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKvnMdgH2Oxv"
   },
   "source": [
    "2. Extract the facial encodings of every person, and name them\n",
    "\n",
    "  The way a face is represented using a set of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "byPB1u4X2Y6H"
   },
   "outputs": [],
   "source": [
    "person1_face_encoding = face_recognition.face_encodings(image_person1)[0]\n",
    "person2_face_encoding = face_recognition.face_encodings(image_person2)[0]\n",
    "person3_face_encoding = face_recognition.face_encodings(image_person3)[0]\n",
    "person4_face_encoding = face_recognition.face_encodings(image_person4)[0]\n",
    "\n",
    "known_face_encodings = [person1_face_encoding,person2_face_encoding,person3_face_encoding,person4_face_encoding]\n",
    "known_face_names = [\"Alex\", \"Hiba\",\"Ban\",\"Ahla Dr.Ahmad\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1EKdaWk2cPA"
   },
   "source": [
    "3. Run the code by detecting face locations and their encodings and labelling them accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Ynx3GyQX2Egk"
   },
   "outputs": [],
   "source": [
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "\n",
    "#To process every other frame for efficiency\n",
    "process_this_frame = True\n",
    "\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    # Resize frame for faster face recognition processing ---> detection on different scale\n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "\n",
    "    # Convert the image from BGR (which OpenCV uses) to RGB (which face_recognition uses)\n",
    "    rgb_small_frame = small_frame[:, :, ::-1]\n",
    "\n",
    "    # Process every other frame to save time (skips 1 frame)\n",
    "    if process_this_frame:\n",
    "        # Find all the faces and face encodings in the current frame\n",
    "        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "        face_names = []\n",
    "        for face_encoding in face_encodings:\n",
    "\n",
    "            # See if the face is a match for the known faces\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "            name = \"Unknown\"\n",
    "\n",
    "            # Use the known face with the smallest distance to the new face\n",
    "            face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "            best_match_index = np.argmin(face_distances)\n",
    "            if matches[best_match_index]:\n",
    "                name = known_face_names[best_match_index]\n",
    "\n",
    "            face_names.append(name)\n",
    "\n",
    "    process_this_frame = not process_this_frame #to skip one frame, it turns this parameter to false\n",
    "\n",
    "    # Display the results\n",
    "    for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "        # Scale back up face locations since the frame we detected in was scaled to 1/4 size (original image)\n",
    "        top *= 4\n",
    "        right *= 4\n",
    "        bottom *= 4\n",
    "        left *= 4\n",
    "\n",
    "        # Draw a rectangle around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "        # Draw a label with a name below the face\n",
    "        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMbrWJ7a1mao"
   },
   "source": [
    "# References:\n",
    "DNN:\n",
    "\n",
    "    - Facial Detection Model:\n",
    "\n",
    "    https://github.com/sr6033/face-detection-with-OpenCV-and-DNN/blob/master/detect_faces_video.py\n",
    "\n",
    "    - Facial Recognition Model:\n",
    "\n",
    "    https://cmusatyalab.github.io/openface/models-and-accuracies/\n",
    "    \n",
    "    http://gitlab.ubocare.com/jiajunjie/face_detect/blob/5891825ac2f2acd8ad8ac31093f1048f4197f722/models/face_detector/openface.nn4.small2.v1.t7\n",
    "\n",
    "Facial Recognition Application:\n",
    "\n",
    "    https://pypi.org/project/face-recognition/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
